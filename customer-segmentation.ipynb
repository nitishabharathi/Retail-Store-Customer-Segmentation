{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [Data Preparation](#1)\n* [Customer Segmentation](#2)\n     * [Clustering the products using k-means algorithm](#3)\n     * [Clustering Analysis](#10)\n     * [Clustering Visualization](#11)\n     * [Clustering the customers using k-means algorithm](#12)\n* [Customer Classification](#25)\n* [RFM Analysis](#25)\n* [Market Basket Analysis](#25)"},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Preparation <a id=\"1\"></a>"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"705714b1-870b-4f34-b5bf-cd027dafaefe","_uuid":"bb40a7b23734d82876d812fab6daecd83a46368c","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime \nimport itertools\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib as mpl\nimport seaborn as sns\nimport squarify\n\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import silhouette_score, confusion_matrix, silhouette_samples\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import precision_score, accuracy_score,recall_score,f1_score\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/ecommerce-data/data.csv\", encoding=\"ISO-8859-1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"Cancelled\"] = df[\"InvoiceNo\"].str.startswith(\"C\")\ndf[\"Cancelled\"] = df[\"Cancelled\"].fillna(False)\n\ndf = df[df[\"Description\"].str.startswith(\"?\") == False]\ndf = df[df[\"Description\"].str.isupper() == True]\ndf = df[df[\"Description\"].str.contains(\"LOST\") == False]\ndf = df[df[\"CustomerID\"].notnull()]\ndf[\"CustomerID\"] = df[\"CustomerID\"].astype(int)\n\ndf['InvoiceNo'].replace(to_replace=\"\\D+\", value=r\"\", regex=True, inplace=True)\ndf['InvoiceNo'] = df['InvoiceNo'].astype('int')\n\ndf = df[(df[\"StockCode\"] != \"DOT\") & (df[\"StockCode\"] != \"POST\")]\ndf.drop(\"StockCode\", inplace=True, axis=1)\n\nqte_false = [74215, 3114, 80995]\nfor qte in qte_false:\n    df = df[(df[\"Cancelled\"] == False) & (df[\"Quantity\"] !=qte)]\n\ndf = df[df[\"Cancelled\"] == False]\ndf.drop(\"Cancelled\", axis=1, inplace=True)\n\ndf[\"TotalCost\"] = df[\"UnitPrice\"] * df[\"Quantity\"]\n\ndf[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n\nbest_buyer = df.groupby([\"Country\", \"InvoiceNo\"])[\"TotalCost\"].sum().reset_index().groupby([\"Country\"])[\"TotalCost\"].mean().sort_values()\nencoder_countries = best_buyer.rank().to_dict()\ndecoder_countries = {i: j for i, j in encoder_countries.items()}\ndf[\"Country\"]  = df[\"Country\"].apply(lambda x:encoder_countries[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Customer Segmentation <a id=\"2\"></a>\n### 2.1 Clustering the products using k-means algorithm <a id=\"3\"></a>"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"62aada7f-1d61-493e-a044-08fcc7bdfb81","_uuid":"4ae364672f6cede623fd0e032e34d967e4f32ee1","trusted":true},"cell_type":"code","source":"is_noun = lambda pos: pos[:2] == 'NN'\n\ndef keywords_inventory(dataframe, column = 'Description'):\n    \n    '''\n    Utility code to fetch the item names from description\n    Returns \n    `keywords`: the list of extracted keywords\n    `keywords_roots`: dictionary with keywords roots as key and words associated with those roots as value\n    `count_keywords`: dictionary with the word as key and the frequency as value\n    '''\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    keywords_roots  = dict()  \n    keywords_select = dict()  \n    category_keys   = []\n    count_keywords  = dict()\n    icount = 0\n    for s in dataframe[column]:\n        if pd.isnull(s): \n            continue\n        lines = s.lower()\n        tokenized = nltk.word_tokenize(lines)\n        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n        \n        for t in nouns:\n            t = t.lower() ; racine = stemmer.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n                count_keywords[racine] += 1                \n            else:\n                keywords_roots[racine] = {t}\n                count_keywords[racine] = 1\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]             \n    return category_keys, keywords_roots, keywords_select, count_keywords\n\ndf_products = pd.DataFrame(df['Description'].unique()).rename(columns = {0:'Description'})\nkeywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_products)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"43300478-3b5a-4c7a-9466-34c384ccae60","_uuid":"5f42482995f36f15688d8cba7f903e1277da5f92","trusted":true},"cell_type":"code","source":"'''\nCreating list of products from the keywords extracted\nCleaning the prodcut names by removing colors, infrequent words\n'''\nproduct_list = []\nfor k,freq in count_keywords.items():\n    word = keywords_select[k]\n    if word in ['pink', 'blue', 'tag', 'green', 'orange']: \n        continue\n    if len(word) < 3 or freq < 13: \n        continue\n    if ('+' in word) or ('/' in word): \n        continue\n    product_list.append([word, freq])\n\nproduct_list.sort(key = lambda x:x[1], reverse = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"23c77363-438b-4694-9b52-960c0ab3aa82","_uuid":"d6faac7eb01d2251fb221569b75e007c8d5146aa","trusted":true},"cell_type":"code","source":"'''\nX is a matrix of products\n'''\n\nproduct_names = df['Description'].unique()\nX = pd.DataFrame()\nfor key, occurence in product_list:\n    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), product_names))\n    \nthreshold = [0, 1, 2, 3, 5, 10]\nlabel_col = []\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n    label_col.append(col)\n    X.loc[:, col] = 0\n\nfor i, prod in enumerate(product_names):\n    prix = df[ df['Description'] == prod]['UnitPrice'].mean()\n    j = 0\n    while prix > threshold[j]:\n        j+=1\n        if j == len(threshold): break\n    X.loc[i, label_col[j-1]] = 1","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"1c81a7c3-8980-4941-9082-7bf5cf92fc14","_uuid":"4ce8586584935e81b9e403ea7a1dd4b2e4c9992e","trusted":true},"cell_type":"code","source":"'''\nClustering the products using K-means Algorithm\n'''\nc = []\nsilhouette_scores = []\nmatrix = X.as_matrix()\n\nfor n_clusters in range(3,10):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    silhouette_scores.append(silhouette_avg)\n    c.append(n_clusters)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### **Silhouette Plot**\nSilhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1]. <br>\nFollowing code taken from [sklearn documentation of Silhouette Plot](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html):"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= (8,4));\nplt.plot( c,silhouette_scores,linestyle ='-',color = 'b');\nplt.title('Silhouette scores for each no of clusters',fontsize=15);\nplt.xlabel('No of Clusters');\nplt.ylabel('Silhouette Score');","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"b29b3bb2-ef8d-4d7f-a19e-52045018ab8e","_uuid":"72dd5bdab528264518bf034b84f66f3c29500fca","trusted":true},"cell_type":"code","source":"n_clusters = 6\nsilhouette_avg = -1\nwhile silhouette_avg < 0.145:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ac1facaf-0be9-412d-bddf-cae0e920bd33","_uuid":"f793937b3c3c83bc14af54d1d00543a2af26034a"},"cell_type":"markdown","source":"### 2.2 Analysing the Clusters <a id=\"4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(clusters, bins=6) # arguments are passed to np.histogram\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of Product per cluster\",fontsize=15)\nplt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Cluster {}\".format(x) for x in range(6)])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() / 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      # Use `label` as label\n        (x_value, y_value),         # Place label at end of the bar\n        xytext=(0, space),          # Vertically shift label by `space`\n        textcoords=\"offset points\", # Interpret `xytext` as offset in points\n        ha='center',                # Horizontally center label\n        va=va)                      # Vertically align label differently for\n                                    # positive and negative values.\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Visualizing each cluster content <a id=\"5\"></a>"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"159600b4-def2-4e4d-be54-6d84938721c4","_uuid":"848009ae647f20366e148eeae96a4aa02f975618","trusted":true},"cell_type":"code","source":"product_list_df = pd.DataFrame(product_names)\nwords_list = [word for (word, occurence) in product_list]\n\noccurence = [dict() for _ in range(n_clusters)]\n\nfor i in range(n_clusters):\n    cluster = product_list_df.loc[clusters == i]\n    for word in words_list:\n        if word in ['art', 'set', 'heart', 'pink', 'blue', 'tag']: \n            continue\n        occurence[i][word] = sum(cluster.loc[:, 0].str.contains(word.upper()))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"2995e126-925b-436f-8b68-55d874637b1e","_uuid":"3d88a32f7998249ce42e267bb912acc7420f0c47","trusted":true},"cell_type":"code","source":"\ndef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    '''\n    Utility function to choose random wordcloud color\n    '''\n    h = int(360.0 * tone / 255.0)\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\ndef make_wordcloud(liste, increment):\n    '''\n    Utility function to plot each cluster wordcloud\n    '''\n    ax1 = fig.add_subplot(4,2,increment)\n    words = dict()\n    trunc_occurences = liste[0:150]\n    for s in trunc_occurences:\n        words[s[0]] = s[1]\n    wordcloud = WordCloud(width=1000,height=400, background_color='white', \n                          max_words=1628,relative_scaling=1,\n                          color_func = random_color_func,\n                          normalize_plurals=False)\n    wordcloud.generate_from_frequencies(words)\n    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n    ax1.axis('off')\n    plt.title('cluster n{}'.format(increment-1))\n\nfig = plt.figure(1, figsize=(14,14))\ncolor = [0, 160, 130, 95, 280, 40, 330, 110, 25]\n\nfor i in range(n_clusters):\n    list_cluster_occurences = occurence[i]\n    tone = color[i] \n    liste = []\n    for key, value in list_cluster_occurences.items():\n        liste.append([key, value])\n    liste.sort(key = lambda x:x[1], reverse = True)\n    make_wordcloud(liste, i+1)            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the clusters we can see that one cluster is associated with christmas items i.e candles, cale,decoration etc. Another cluster is associated with luxuary items like necklace, silver, bracelet etc. Another cluster realted to vintage stuffs like retro, polkadot, retrospot. But it is observed that a few words occur in more than one cluster which make it tough to distinguish. In order to ensure these clusters are distinct Principal Component Analysis is performed."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"41cd8738-4923-43cd-b26a-fba6d53070e8","_uuid":"abce52d76e801aa6197603925618fb032aec78af","trusted":true},"cell_type":"code","source":"'''\nPerforming PCA and checking varience explained by each component\n'''\npca = PCA()\npca.fit(matrix)\npca_samples = pca.transform(matrix)\n\nfig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid')\n\nplt.xlim(0, 100)\n\n#ax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('No of Principal components', fontsize = 14)\nplt.legend(loc='upper left', fontsize = 13);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29b5ae8a-7430-47b6-a4cd-991dbb3073ca","_uuid":"159540979f024ddac04749daace71e737fc558a1"},"cell_type":"markdown","source":"Requires more than 100 components to explain 90% of the variance of the data"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"0e49b019-d1b7-4815-8c5e-7cf27fa5b5b2","_uuid":"fb0afe8523c2634860fdaf67d734c1dc0897c4c0","trusted":true},"cell_type":"code","source":"pca = PCA(n_components=50)\nmatrix_9D = pca.fit_transform(matrix)\nmat = pd.DataFrame(matrix_9D)\nmat['cluster'] = pd.Series(clusters)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"675cc670-4512-4983-8b58-e82f3cf2bf3d","_uuid":"b9d872fa5038458f3424dfc585a5a823efc7ff7f","trusted":true},"cell_type":"code","source":"\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (15,8))\nincrement = 0\nfor ix in range(4):\n    for iy in range(ix+1, 4):    \n        increment += 1\n        ax = fig.add_subplot(2,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 9: break\n    if increment == 9: break\n        \ncomp_handler = []\nfor i in range(5):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n           title='Cluster', facecolor = 'lightgrey',\n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2)\nproj = tsne.fit_transform(X)\n\nplt.figure(figsize=(8,8))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"TSNE Visulalization\", fontsize=\"15\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_article_to_cluster = {article : cluster for article, cluster in zip(product_names, clusters)}\ncluster = df['Description'].apply(lambda x : dict_article_to_cluster[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Clustering customers <a id='6'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nData preparation for the customer clustering\n'''\ncluster = df['Description'].apply(lambda x : dict_article_to_cluster[x])\ndf2 = pd.get_dummies(cluster, prefix=\"Article_cluster\").mul(df[\"TotalCost\"], 0)\ndf2 = pd.concat([df['InvoiceNo'], df2], axis=1)\ndf2_grouped = df2.groupby('InvoiceNo').sum()\n\ncustom_aggregation = {}\ncustom_aggregation[\"TotalCost\"] = \"sum\"\ncustom_aggregation[\"InvoiceDate\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"CustomerID\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"Country\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"Quantity\"] = \"sum\"\n\ndf_grouped = df.groupby(\"InvoiceNo\").agg(custom_aggregation)\n\n\nnow = df_grouped[\"InvoiceDate\"].max() \ndf_grouped[\"Recency\"] = now - df_grouped[\"InvoiceDate\"]\ndf_grouped[\"Recency\"] = pd.to_timedelta(df_grouped[\"Recency\"]).astype(\"timedelta64[D]\") # conversion to day from now\n\ndf_grouped[\"nb_visit\"] = 1\ndf_grouped[\"total_spent\"] = 1\n\ndf2_grouped_final = pd.concat([df_grouped['CustomerID'], df2_grouped], axis=1).set_index(\"CustomerID\").groupby(\"CustomerID\").sum()\ndf2_grouped_final = df2_grouped_final.div(df2_grouped_final.sum(axis=1), axis=0)\ndf2_grouped_final = df2_grouped_final.fillna(0)\n\ncustom_aggregation = {}\ncustom_aggregation[\"TotalCost\"] = [\"mean\", \"sum\"]\ncustom_aggregation[\"nb_visit\"] = \"sum\"\ncustom_aggregation[\"Country\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"Quantity\"] = \"sum\"\ncustom_aggregation[\"Recency\"] = [\"min\", \"max\"]\n\ndf_grouped_final = df_grouped.groupby(\"CustomerID\").agg(custom_aggregation)\ndf_grouped_final[\"Freq\"] = (df_grouped_final[\"Recency\"][\"max\"]  - df_grouped_final[\"Recency\"][\"min\"] ) / df_grouped_final[\"nb_visit\"][\"sum\"]\ndf_grouped_final.columns = [\"avg_price\", \"sum_price\", \"nb_visit\", \"country\", \"quantity\", \"min_recency\", \"max_recency\", \"freq\"]\n\nX1 = df_grouped_final.as_matrix()\nX2 = df2_grouped_final.as_matrix()\n\nscaler = StandardScaler()\nX1 = scaler.fit_transform(X1)\nX_final_std_scale = np.concatenate((X1, X2), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(init='k-means++', n_clusters = 6, n_init=30, random_state=25)  \nkmeans.fit(X_final_std_scale)\nclusters = kmeans.predict(X_final_std_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,8))\nn, bins, patches = plt.hist(clusters, bins=6) # arguments are passed to np.histogram\nplt.xlabel(\"Cluster\")\nplt.title(\"Number of Customer per cluster\")\nplt.xticks([rect.get_x()+ rect.get_width() / 2 for rect in patches], [\"Cluster {}\".format(x) for x in range(6)])\n\nfor rect in patches:\n    y_value = rect.get_height()\n    x_value = rect.get_x() + rect.get_width() / 2\n\n    space = 5\n    va = 'bottom'\n    label = str(int(y_value))\n    \n    plt.annotate(\n        label,                      # Use `label` as label\n        (x_value, y_value),         # Place label at end of the bar\n        xytext=(0, space),          # Vertically shift label by `space`\n        textcoords=\"offset points\", # Interpret `xytext` as offset in points\n        ha='center',                # Horizontally center label\n        va=va)                      # Vertically align label differently for\n                                    # positive and negative values.\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components=2)\nproj = tsne.fit_transform(X_final_std_scale)\n\nplt.figure(figsize=(10,10))\nplt.scatter(proj[:,0], proj[:,1], c=clusters)\nplt.title(\"TSNE Visualization\", fontsize=\"15\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_grouped_final[\"cluster\"] = clusters\ndf_grouped_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_analysis = df_grouped_final.groupby(\"cluster\").mean()\ndf_analysis.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price = df_analysis[\"avg_price\"].values\nfreq = df_analysis[\"freq\"].values\nvisit = df_analysis[\"nb_visit\"].values\n\nplt.figure(figsize = (10,6))\nplt.scatter(price, freq, s=visit*20)\n    \nfor label, x, y in zip([\"Customer #{}\".format(x) for x in range(6)], price, freq):\n    plt.annotate(\n        label,\n        xy=(x, y), xytext=(-20, 20),\n        textcoords='offset points', ha='right', va='bottom',\n        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n    \nplt.title(\"Return Time and Frequency of Purchase per Customer Segment\",fontsize=15)\nplt.xlabel(\"Average Price per Invoice\")\nplt.ylabel(\"Time between 2 invoices\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Customer Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_dataset = pd.concat([df_grouped_final, df2_grouped_final], axis = 1)\nclassification_dataset.head()\n\nX = classification_dataset.drop(\"cluster\", axis=1).as_matrix()\ny = classification_dataset[\"cluster\"].as_matrix()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluation(acc,Y_prediction,Y_test):\n    '''\n    Utility function to compute evaluation measures\n    '''\n    accuracy.append(acc)\n    precision.append(precision_score(Y_prediction, Y_test,average='micro'))\n    recall.append(recall_score(Y_prediction, Y_test, average='micro'))\n    f1.append(f1_score(Y_prediction, Y_test,average='micro'))\n    \ndef printResult():\n    '''\n    Utility function to print the results in tabular format\n    '''\n    dict = {'Model': models, 'Accuracy': accuracy,'F1 score':f1}  \n    \n    df = pd.DataFrame(dict)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, Y_test = train_test_split(X, y, test_size=0.25)\n\nmodels= []\naccuracy = []\nprecision = []\nrecall = []\nf1 = []\n\nmodels.append('Decision Tree')\nclf1 = DecisionTreeClassifier(random_state=25)\nclf1.fit(X_train, y_train)\ny_pred1 = clf1.predict(X_test) \nacc = round(clf1.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred1,Y_test)\n\nmodels.append('Random Forest')\nclf2 = RandomForestClassifier(n_estimators=100,random_state=25)\nclf2.fit(X_train, y_train)\ny_pred2 = clf2.predict(X_test)\nacc = round(clf2.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred2,Y_test)\n\nmodels.append('SVM')\nclf3 = SVC()\nclf3.fit(X_train, y_train)\ny_pred3 = clf3.predict(X_test)\nacc = round(clf3.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred3,Y_test)\n\nmodels.append('Logistic Regression')\nclf4 = LogisticRegression()\nclf4.fit(X_train, y_train)\ny_pred4 = clf4.predict(X_test)\nacc = round(clf4.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred4,Y_test)\n\nmodels.append('Naive Bayes')\nclf5 = GaussianNB()\nclf5.fit(X_train, y_train)\ny_pred5 = clf5.predict(X_test)\nacc = round(clf5.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred5,Y_test)\n\nmodels.append('XG Boost')\nclf6 =  XGBClassifier()\nclf6.fit(X_train, y_train)\ny_pred6 = clf6.predict(X_test)\nacc = round(clf6.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred6,Y_test)\n\nmodels.append('Ada Boost')\nclf7 = AdaBoostClassifier()\nclf7.fit(X_train, y_train)\ny_pred7 = clf7.predict(X_test)\nacc = round(clf7.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred7,Y_test)\n\nmodels.append('Light GBM')\nclf8 = lgb.LGBMClassifier()\nclf8.fit(X_train, y_train)\ny_pred8 = clf8.predict(X_test)\nacc = round(clf8.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred8,Y_test)\n\nmodels.append('KNN')\nclf9 = KNeighborsClassifier(n_neighbors = 3)\nclf9.fit(X_train, y_train)\ny_pred9 = clf9.predict(X_test)\nacc = round(clf9.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred9,Y_test)\n\nmodels.append('Perceptron')\nclf10 = Perceptron()\nclf10.fit(X_train, y_train)\ny_pred10 = clf10.predict(X_test)\nacc = round(clf10.score(X_test, Y_test) * 100, 2)\nevaluation(acc,y_pred10,Y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printResult()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n#     plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclasses = [\"Cluster {}\".format(x) for x in range(6)]\nnp.set_printoptions(precision=2)\n\nplt.figure(figsize=(10,20))\nplt.subplot(5, 2, 1)\ncnf_matrix = confusion_matrix(Y_test, y_pred1)\nplot_confusion_matrix(cnf_matrix, classes=classes, title='DecisionTreeClassifier')\n\nplt.subplot(5, 2, 2)\ncnf_matrix = confusion_matrix(Y_test, y_pred2)\nplot_confusion_matrix(cnf_matrix, classes=classes, title='RandomForestClassifier')\n\nplt.subplot(5, 2, 3)\ncnf_matrix = confusion_matrix(Y_test, y_pred3)\nplot_confusion_matrix(cnf_matrix, classes=classes, title='SVM')\n\nplt.subplot(5, 2, 4)\ncnf_matrix = confusion_matrix(Y_test, y_pred4)\nplot_confusion_matrix(cnf_matrix, classes=classes, title=' Logistic Regression')\n\nplt.subplot(5, 2, 5)\ncnf_matrix = confusion_matrix(Y_test, y_pred5)\nplot_confusion_matrix(cnf_matrix, classes=classes, title='Naive Bayes')\n\nplt.subplot(5, 2, 6)\ncnf_matrix = confusion_matrix(Y_test, y_pred6)\nplot_confusion_matrix(cnf_matrix, classes=classes, title='XG Boost')\n\nplt.subplot(5, 2, 7)\ncnf_matrix = confusion_matrix(Y_test, y_pred7)\nplot_confusion_matrix(cnf_matrix, classes=classes, title='AdaBoost')\n\nplt.subplot(5, 2, 8)\ncnf_matrix = confusion_matrix(Y_test, y_pred8)\nplot_confusion_matrix(cnf_matrix, classes=classes, title=' Ligth GBM')\n\n#plt.subplot(5, 2, 9)\nprint('tgr')\ncnf_matrix = confusion_matrix(Y_test, y_pred9)\n#plot_confusion_matrix(cnf_matrix, classes=classes, title='KNN')\n\n#plt.subplot(5, 2, 10)\ncnf_matrix = confusion_matrix(Y_test, y_pred10)\n#plot_confusion_matrix(cnf_matrix, classes=classes, title='Perceptron')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RFM Analysis\n\nRFM analysis is a data-driven customer behavior segmentation technique where RFM stands for recency, frequency, and monetary value. The idea is to segment customers based on when their last purchase was(Recency), how often they’ve purchased in the past(Frequency), and how much they spent(Monetary). All three of these measures have proven to be effective predictors of a customer’s willingness to engage in marketing messages and offers. <br>\n\n**Reference:** [RFM Analysis in Python](https://medium.com/capillary-data-science/rfm-analysis-an-effective-customer-segmentation-technique-using-python-58804480d232)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\n\ntoday = dt.datetime(2012,1,1)\ndata_x = df.groupby('CustomerID').agg({'TotalCost': lambda x: x.sum(),\n                                        'InvoiceDate': lambda x: (today - x.max()).days})\ndata_y = df.groupby(['CustomerID','InvoiceNo']).agg({'TotalCost': lambda x: x.sum()})\n\ndata_z = data_y.groupby('CustomerID').agg({'TotalCost': lambda x: len(x)})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCreate RFM Table\n'''\nrfm_table= pd.merge(data_x,data_z, on='CustomerID')\nrfm_table.rename(columns= {'InvoiceDate': 'Recency',\n                          'TotalCost_y': 'Frequency',\n                          'TotalCost_x': 'Monetary'}, inplace= True)\nrfm_table.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Frequency bulma\ndef FScore(x,p,d):\n    if x <= d[p][0.20]:\n        return 0\n    elif x <= d[p][0.40]:\n        return 1\n    elif x <= d[p][0.60]: \n        return 2\n    elif x <= d[p][0.80]:\n        return 3\n    else:\n        return 4\n\nquantiles = rfm_table.quantile(q=[0.20,0.40,0.60,0.80])\nquantiles = quantiles.to_dict()\nrfm_table['Freq_Tile'] = rfm_table['Frequency'].apply(FScore, args=('Frequency',quantiles,))\n \nrfm_table = rfm_table.sort_values('Recency',ascending=True)\nrfm_table['Rec_Tile'] = pd.qcut(rfm_table['Recency'],5,labels=False)\n\nrfm_table['Mone_Tile'] = pd.qcut(rfm_table['Monetary'],5,labels=False)\n\nrfm_table['Rec_Tile'] = rfm_table['Rec_Tile'] + 1\nrfm_table['Freq_Tile'] = rfm_table['Freq_Tile'] + 1\nrfm_table['Mone_Tile'] = rfm_table['Mone_Tile'] + 1\n\nrfm_table['RFM Score'] = rfm_table['Rec_Tile'].map(str) + rfm_table['Freq_Tile'].map(str) + rfm_table['Mone_Tile'].map(str)\nrfm_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_table['RFM_Sum'] = rfm_table[['Freq_Tile','Rec_Tile','Mone_Tile']].sum(axis=1)\nrfm_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Define rfm_level function\ndef rfm_level(df):\n    if df['RFM_Sum'] >= 9:\n        return 'Core'\n    elif ((df['RFM_Sum'] >= 8) and (df['RFM_Sum'] < 9)):\n        return 'Loyal'\n    elif ((df['RFM_Sum'] >= 7) and (df['RFM_Sum'] < 8)):\n        return 'Whales'\n    elif ((df['RFM_Sum'] >= 6) and (df['RFM_Sum'] < 7)):\n        return 'Rookies'\n    elif ((df['RFM_Sum'] >= 5) and (df['RFM_Sum'] < 6)):\n        return 'Slipping'\n    else:\n        return 'Regular'\n\nrfm_table['RFM_Level'] = rfm_table.apply(rfm_level, axis=1)\nrfm_table.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_table[\"RFM_Level\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_level_agg = rfm_table.groupby('RFM_Level').agg({\n    'Recency': 'mean',\n    'Frequency': 'mean',\n    'Monetary': ['mean', 'count']}).round(1)\nprint(rfm_level_agg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrfm_level_agg.columns = ['RecencyMean','FrequencyMean','MonetaryMean', 'Count']\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(16, 9)\nsquarify.plot(sizes=rfm_level_agg['Count'], \n              label=['Core',\n                     'Loyal',\n                     'Whales',\n                     'Rookies',\n                     'Slipping', \n                     'Regular'], alpha=.6 )\nplt.title(\"RFM Segments\",fontsize=18,fontweight=\"bold\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Market Basket Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting data to binary format suitable for Apriori\ndata_apr = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\ndef num(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n\nbasket_new = data_apr.applymap(num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items = apriori(basket_new, min_support=0.02, use_colnames=True)\nrule = association_rules(items, metric=\"lift\", min_threshold=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rule['consequents'] = rule['consequents'].apply(lambda x: list(x))\nrule['antecedents'] = rule['antecedents'].apply(lambda x: list(x))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}